*** Reading local file: /root/airflow/logs/lesson1.solution6/create_table/2021-11-01T17:48:46.382903+00:00/1.log
[2021-11-01 17:48:53,881] {models.py:1359} INFO - Dependencies all met for <TaskInstance: lesson1.solution6.create_table 2021-11-01T17:48:46.382903+00:00 [queued]>
[2021-11-01 17:48:53,898] {models.py:1359} INFO - Dependencies all met for <TaskInstance: lesson1.solution6.create_table 2021-11-01T17:48:46.382903+00:00 [queued]>
[2021-11-01 17:48:53,898] {models.py:1571} INFO - 
--------------------------------------------------------------------------------
Starting attempt 1 of 1
--------------------------------------------------------------------------------

[2021-11-01 17:48:53,950] {models.py:1593} INFO - Executing <Task(PostgresOperator): create_table> on 2021-11-01T17:48:46.382903+00:00
[2021-11-01 17:48:53,950] {base_task_runner.py:118} INFO - Running: ['bash', '-c', 'airflow run lesson1.solution6 create_table 2021-11-01T17:48:46.382903+00:00 --job_id 2 --raw -sd DAGS_FOLDER/lesson1_solutions/solution6.py --cfg_path /tmp/tmp87apru3d']
[2021-11-01 17:48:54,741] {base_task_runner.py:101} INFO - Job 2: Subtask create_table [2021-11-01 17:48:54,740] {settings.py:174} INFO - settings.configure_orm(): Using pool settings. pool_size=5, pool_recycle=1800, pid=2876
[2021-11-01 17:48:55,615] {base_task_runner.py:101} INFO - Job 2: Subtask create_table [2021-11-01 17:48:55,614] {__init__.py:51} INFO - Using executor LocalExecutor
[2021-11-01 17:48:56,078] {base_task_runner.py:101} INFO - Job 2: Subtask create_table [2021-11-01 17:48:56,077] {models.py:273} INFO - Filling up the DagBag from /home/workspace/airflow/dags/lesson1_solutions/solution6.py
[2021-11-01 17:48:56,337] {base_task_runner.py:101} INFO - Job 2: Subtask create_table [2021-11-01 17:48:56,337] {cli.py:520} INFO - Running <TaskInstance: lesson1.solution6.create_table 2021-11-01T17:48:46.382903+00:00 [running]> on host cf977f341b36
[2021-11-01 17:48:56,433] {postgres_operator.py:57} INFO - Executing: 
CREATE TABLE IF NOT EXISTS trips (
trip_id INTEGER NOT NULL,
start_time TIMESTAMP NOT NULL,
end_time TIMESTAMP NOT NULL,
bikeid INTEGER NOT NULL,
tripduration DECIMAL(16,2) NOT NULL,
from_station_id INTEGER NOT NULL,
from_station_name VARCHAR(100) NOT NULL,
to_station_id INTEGER NOT NULL,
to_station_name VARCHAR(100) NOT NULL,
usertype VARCHAR(20),
gender VARCHAR(6),
birthyear INTEGER,
PRIMARY KEY(trip_id))
DISTSTYLE ALL;
[2021-11-01 17:48:56,529] {logging_mixin.py:95} INFO - [2021-11-01 17:48:56,528] {base_hook.py:83} INFO - Using connection to: id: redshift. Host: redshift-cluster.xxxxxxx.us-east-2.redshift.amazonaws.com, Port: 5439, Schema: dev, Login: airflow_redshift_user, Password: XXXXXXXX, extra: {}
[2021-11-01 17:51:07,092] {models.py:1788} ERROR - could not connect to server: Connection timed out
	Is the server running on host "redshift-cluster.xxxxxxxxx.us-east-2.redshift.amazonaws.com" (3.133.189.56) and accepting
	TCP/IP connections on port 5439?
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/site-packages/airflow/models.py", line 1657, in _run_raw_task
    result = task_copy.execute(context=context)
  File "/opt/conda/lib/python3.6/site-packages/airflow/operators/postgres_operator.py", line 60, in execute
    self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
  File "/opt/conda/lib/python3.6/site-packages/airflow/hooks/dbapi_hook.py", line 158, in run
    with closing(self.get_conn()) as conn:
  File "/opt/conda/lib/python3.6/site-packages/airflow/hooks/postgres_hook.py", line 60, in get_conn
    self.conn = psycopg2.connect(**conn_args)
  File "/opt/conda/lib/python3.6/site-packages/psycopg2/__init__.py", line 130, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: could not connect to server: Connection timed out
	Is the server running on host "redshift-cluster.xxxxxxxx.us-east-2.redshift.amazonaws.com" (3.133.189.56) and accepting
	TCP/IP connections on port 5439?

[2021-11-01 17:51:07,101] {models.py:1819} INFO - Marking task as FAILED.
[2021-11-01 17:51:07,134] {base_task_runner.py:101} INFO - Job 2: Subtask create_table Traceback (most recent call last):
[2021-11-01 17:51:07,134] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/bin/airflow", line 32, in <module>
[2021-11-01 17:51:07,134] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     args.func(args)
[2021-11-01 17:51:07,134] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/utils/cli.py", line 74, in wrapper
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     return f(*args, **kwargs)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/bin/cli.py", line 526, in run
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     _run(args, dag, ti)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/bin/cli.py", line 445, in _run
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     pool=args.pool,
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/utils/db.py", line 73, in wrapper
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     return func(*args, **kwargs)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/models.py", line 1657, in _run_raw_task
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     result = task_copy.execute(context=context)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/operators/postgres_operator.py", line 60, in execute
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     self.hook.run(self.sql, self.autocommit, parameters=self.parameters)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/hooks/dbapi_hook.py", line 158, in run
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     with closing(self.get_conn()) as conn:
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/airflow/hooks/postgres_hook.py", line 60, in get_conn
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     self.conn = psycopg2.connect(**conn_args)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table   File "/opt/conda/lib/python3.6/site-packages/psycopg2/__init__.py", line 130, in connect
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table     conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
[2021-11-01 17:51:07,135] {base_task_runner.py:101} INFO - Job 2: Subtask create_table psycopg2.OperationalError: could not connect to server: Connection timed out
[2021-11-01 17:51:07,136] {base_task_runner.py:101} INFO - Job 2: Subtask create_table 	Is the server running on host "redshift-cluster.xxxxxxxx.us-east-2.redshift.amazonaws.com" (3.133.189.56) and accepting
[2021-11-01 17:51:07,136] {base_task_runner.py:101} INFO - Job 2: Subtask create_table 	TCP/IP connections on port 5439?
[2021-11-01 17:51:07,136] {base_task_runner.py:101} INFO - Job 2: Subtask create_table 
[2021-11-01 17:51:09,445] {logging_mixin.py:95} INFO - [2021-11-01 17:51:09,444] {jobs.py:2527} INFO - Task exited with return code 1